{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "import msiwarp as mx\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tqdm import tqdm\n",
    "from msiwarp.util.warp import peak_density_mz\n",
    "from bisect import bisect_left, bisect_right\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "from src.psalign.mass_dispersion import get_mass_dispersion\n",
    "\n",
    "def printf(x):\n",
    "    print(f'Average mass dispersion [ppm]: {np.format_float_positional(x[0], 2)}')\n",
    "    print(f'Median mass dispersion [ppm]: {np.format_float_positional(x[1], 2)}')\n",
    "    print(f'Average cosine similarity: {np.format_float_positional(x[2], 4)}')\n",
    "\n",
    "def interp(X, x, y):\n",
    "    f = make_interp_spline(x, y, k=1)\n",
    "    return f(X).astype(X.dtype)\n",
    "\n",
    "def cossim(a, b):\n",
    "    return np.dot(a / np.linalg.norm(a), b / np.linalg.norm(b))\n",
    "\n",
    "def perform_pwl_warping(mz_vector, data, reference, warping_knots):\n",
    "    # invert\n",
    "    interpolated_mz = interp(mz_vector, np.array([w[0] for w in warping_knots]), np.array([w[0] + w[1] for w in warping_knots]))\n",
    "    invert = binning(mz_vector, interpolated_mz, data)\n",
    "    interpolated_mz = interp(mz_vector, np.array([w[0] for w in warping_knots]), np.array([w[0] - w[1] for w in warping_knots]))\n",
    "    not_invert = binning(mz_vector, interpolated_mz, data)\n",
    "    \n",
    "    if cossim(reference, invert) > cossim(reference, not_invert):\n",
    "        return invert\n",
    "    return not_invert\n",
    "\n",
    "@nb.njit\n",
    "def binning(x_correct, x_wrong, values):\n",
    "    \n",
    "    index_min = np.argmin(np.abs(x_wrong - x_correct[0]))\n",
    "    if x_wrong[index_min] > x_correct[0]:\n",
    "        index_min -= 1\n",
    "        index_min = index_min if index_min > 0 else 0\n",
    "    index_max = np.argmin(np.abs(x_wrong - x_correct[-1]))\n",
    "    if x_wrong[index_max] < x_correct[-1]:\n",
    "        index_max += 1\n",
    "        index_max = index_max if index_max < x_wrong.shape[0] else x_wrong.shape[0]\n",
    "        \n",
    "    result = np.zeros_like(x_correct, dtype=values.dtype)\n",
    "    \n",
    "    values = values[index_min: index_max + 1]\n",
    "    x_wrong = x_wrong[index_min: index_max + 1]\n",
    "    \n",
    "    indices = np.searchsorted(x_correct, x_wrong, side='right') - 1\n",
    "    \n",
    "    idxs = indices < 0\n",
    "    if idxs.sum() > 0:\n",
    "        result[0] = np.dot(values[idxs], x_wrong[idxs] - x_correct[0]) / (x_correct[0] - x_correct[1])\n",
    "    \n",
    "    idxs = np.logical_and(0 <= indices, indices < x_correct.shape[0] - 1)\n",
    "    if idxs.sum() > 0:\n",
    "        temp = indices[idxs]\n",
    "        shifted = temp + 1\n",
    "        factor = np.divide(x_wrong[idxs] - x_correct[temp], x_correct[shifted] - x_correct[temp])\n",
    "        vals = values[idxs]\n",
    "        for i, index in enumerate(temp):\n",
    "            result[index] += vals[i] * (1 - factor[i])\n",
    "            result[index + 1] += vals[i] * factor[i]\n",
    "    \n",
    "    idxs = indices >= x_correct.shape[0] - 1\n",
    "    if idxs.sum() > 0:\n",
    "        temp = indices[idxs]\n",
    "        result[-1] += np.dot(values[idxs], np.divide(x_wrong[idxs] - x_correct[temp], x_correct[temp] - x_correct[temp - 1]))\n",
    "                \n",
    "    return result\n",
    "\n",
    "def centroid_tof(spectrum, peak_threshold, window_size = 11, order = 2, distance=None):\n",
    "    intensity_golay = savgol_filter(np.array(spectrum[1]), window_size, order)\n",
    "    intensity_golay2 = savgol_filter(intensity_golay, 11, order)\n",
    "    # Centroid smoothed signal\n",
    "    (mz_c, intensity_c) = parabolic_centroid(np.array(spectrum[0]), intensity_golay2, peak_threshold, distance)\n",
    "    return (mz_c, intensity_c)\n",
    "\n",
    "def parabolic_centroid(mzs, intensities, peak_threshold, distance=None):\n",
    "    peak_indices, _ = find_peaks(intensities, height=peak_threshold, distance=distance)\n",
    "    peak_left = peak_indices - 1\n",
    "    peak_right = peak_indices + 1\n",
    "    \n",
    "    n = len(peak_indices)\n",
    "    \n",
    "    X = np.zeros((n, 3))\n",
    "    Y = np.zeros((n, 3))\n",
    "    \n",
    "    X[:,0] = mzs[peak_left]\n",
    "    X[:,1] = mzs[peak_indices]\n",
    "    X[:,2] = mzs[peak_right]\n",
    "    \n",
    "    Y[:,0] = intensities[peak_left]\n",
    "    Y[:,1] = intensities[peak_indices]\n",
    "    Y[:,2] = intensities[peak_right]\n",
    "    \n",
    "    a = ((Y[:,2] - Y[:,1]) / (X[:,2] - X[:,1]) - \n",
    "         (Y[:,1] - Y[:,0]) / (X[:,1] - X[:,0])) / (X[:,2] - X[:,0])\n",
    "    \n",
    "    b = ((Y[:,2] - Y[:,1]) / (X[:,2] - X[:,1]) * (X[:,1] - X[:,0]) + \n",
    "         (Y[:,1] - Y[:,0]) / (X[:,1] - X[:,0]) * (X[:,2] - X[:,1])) / (X[:,2] - X[:,0])             \n",
    "\n",
    "    mzs_parabolic = ((1/2) * (-b + 2 * a * X[:,1]) / a)\n",
    "    intensities_parabolic = (a * (mzs_parabolic - X[:,1]) ** 2 +\n",
    "                             b * (mzs_parabolic - X[:,1]) + Y[:,1])\n",
    "    \n",
    "    return (mzs_parabolic, intensities_parabolic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ea63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file from https://www.ebi.ac.uk/pride/archive/projects/PXD013069\n",
    "sample = 'drugtreatedspheroids-nonormalization'\n",
    "\n",
    "path = '<path_to_data>/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dcc7af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of peaks: 21.29\n"
     ]
    }
   ],
   "source": [
    "# Conversion happens when tof_spheroids.ipynb is run\n",
    "file = np.load(f\"{path}/{sample}.npz\")\n",
    "mz_vector = file['axis']\n",
    "row2grid = file['location']\n",
    "maldi = file['data']\n",
    "\n",
    "del file\n",
    "\n",
    "nb_peaks = 50\n",
    "start_maldi_alignment = 800\n",
    "stop_maldi_alignment = 4500\n",
    "start_index_alignment = np.max([bisect_left(mz_vector, start_maldi_alignment) - 1, 0])\n",
    "stop_index_alignment = np.min([bisect_right(mz_vector, stop_maldi_alignment), mz_vector.shape[0] - 1])\n",
    "maldi = maldi[:, start_index_alignment: stop_index_alignment + 1]\n",
    "mz_vector = mz_vector[start_index_alignment: stop_index_alignment + 1]\n",
    "\n",
    "l2 = np.linalg.norm(maldi, axis=1)\n",
    "tic = np.sum(maldi, axis=1)\n",
    "for i in range(maldi.shape[0]):\n",
    "    maldi[i, :] /= maldi[i, :].sum()\n",
    "\n",
    "distance = 5\n",
    "threshold = 99.9\n",
    "reference = maldi[np.argmax(tic)]\n",
    "\n",
    "maldi = np.load(f\"{path}/{sample}.npz\")['data'][:, start_index_alignment: stop_index_alignment + 1]\n",
    "\n",
    "# scaling to test impact of sigma on alignment performance\n",
    "sigma_1 = 4e-5\n",
    "epsilon = 2.55\n",
    "bandwidth = 100\n",
    "slack = 2.0 * epsilon * sigma_1\n",
    "\n",
    "a = np.stack([mz_vector, reference], axis=0)\n",
    "\n",
    "refe = centroid_tof(a, peak_threshold=np.percentile(reference, threshold), distance=distance)\n",
    "\n",
    "spect_matrix = []\n",
    "\n",
    "for i in range(maldi.shape[0]):\n",
    "    a = np.stack([mz_vector, maldi[i, :]], axis=0)\n",
    "\n",
    "    spect_matrix.append(centroid_tof(a, peak_threshold=np.percentile(maldi[i, :], threshold), distance=distance))\n",
    "    \n",
    "size = list(map(lambda x: len(x[0]), spect_matrix))\n",
    "print(\"Average number of peaks:\", np.format_float_positional(np.mean(size), 2))\n",
    "    \n",
    "size = maldi.shape\n",
    "dtype = maldi.dtype\n",
    "del maldi\n",
    "\n",
    "ref = [mx.peak(i, mz_i, h_i, sigma_1 * mz_i) for i, (mz_i, h_i) in enumerate(zip(refe[0], refe[1]))]\n",
    "spec_matrix = [[mx.peak(i, mz_i, h_i, sigma_1 * mz_i) for i, (mz_i, h_i) in enumerate(zip(spect_m[0], spect_m[1]))] for spect_m in spect_matrix]\n",
    "del spect_matrix\n",
    "\n",
    "# ---------- find peak dense regions across data set spectra ----------\n",
    "mz_begin = start_maldi_alignment\n",
    "mz_end = stop_maldi_alignment\n",
    "xi = np.linspace(mz_begin, mz_end, 4000)\n",
    "(yi, xp, yp) = peak_density_mz(spec_matrix, xi, bandwidth=bandwidth, stride=5)\n",
    "\n",
    "# we're using the same warping nodes for all spectra here\n",
    "node_mzs = (xp[:-1] + xp[1:]) / 2\n",
    "node_mzs = np.array([mz_begin, *node_mzs, mz_end])\n",
    "\n",
    "# setup warping parameters \n",
    "n_steps = 50 # the slack of a warping node is +- (n_steps * s * sigma @ the node's m/z)\n",
    "\n",
    "node_slacks = np.array([slack * mz for mz in node_mzs])\n",
    "nodes = mx.initialize_nodes(node_mzs, node_slacks, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584f3734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.48s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "warping_functions = mx.find_optimal_spectra_warpings(spec_matrix, ref, nodes, epsilon)\n",
    "warped_spectra = [mx.warp_peaks(s_i, nodes, o_i) for (s_i, o_i) in zip(spec_matrix, warping_functions)]\n",
    "t1 = time.time()\n",
    "print(\"time: {:0.2f}s\".format(t1 - t0))\n",
    "\n",
    "del spec_matrix\n",
    "del warped_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232fe77",
   "metadata": {},
   "source": [
    "The computation time is very low for MSIWarp if the number of peaks present are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8971649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before alignment:\n",
      "Average mass dispersion [ppm]: 19.53\n",
      "Median mass dispersion [ppm]: 19.02\n",
      "Average cosine similarity: 0.6532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1201/1201 [00:21<00:00, 57.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After alignment:\n",
      "Average mass dispersion [ppm]: 12.88\n",
      "Median mass dispersion [ppm]: 12.22\n",
      "Average cosine similarity: 0.7941\n"
     ]
    }
   ],
   "source": [
    "warping_funcs = [[(nodes[o].mz, nodes[o].mz_shifts[f[o]]) for o in range(len(nodes))] for f in warping_functions]\n",
    "\n",
    "with mp.Pool(processes=mp.cpu_count() // 2) as pool:\n",
    "    \n",
    "    maldi = np.load(f\"{path}/{sample}.npz\")['data'][:, start_index_alignment: stop_index_alignment + 1]\n",
    "    res = np.empty_like(maldi)\n",
    "    \n",
    "    print(\"Before alignment:\")\n",
    "    printf(get_mass_dispersion(maldi, mz_vector, pool=pool, nb_of_peaks=nb_peaks))\n",
    "    \n",
    "    for i in tqdm(range(maldi.shape[0])):\n",
    "        res[i] = perform_pwl_warping(mz_vector, maldi[i, :], reference, warping_funcs[i])\n",
    "    \n",
    "    print(\"\\nAfter alignment:\")\n",
    "    printf(get_mass_dispersion(res, mz_vector, pool=pool, nb_of_peaks=nb_peaks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
